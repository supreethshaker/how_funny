{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3263c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T23:31:01.083610Z",
     "start_time": "2021-11-24T23:30:57.151754Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ee79a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T23:31:01.100391Z",
     "start_time": "2021-11-24T23:31:01.085565Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    print(\"Loading Data...\", end='\\r')\n",
    "    df = pd.read_csv(\"cleaned_data.csv\")\n",
    "    df.drop(columns=df.columns.values[0:2], inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df_jokes = df[df['joke'] > 0]\n",
    "    df_non_jokes = df[df['joke'] < 0]\n",
    "    df = pd.concat([df_jokes.sample(25000, random_state=42),\n",
    "                    df_non_jokes.sample(25000, random_state=42)])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['cleaned text'], df['joke'], test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    print(\"Building Vocab from training data...\", end='\\r')\n",
    "    X_train = [doc.split() for doc in X_train]\n",
    "    X_test = [doc.split() for doc in X_test]\n",
    "    w2v = Word2Vec(size=200, window=5, min_count=4,\n",
    "                   workers=8)  # vector_size = size\n",
    "    # w2v = Word2Vec(vector_size=200, window=5, min_count=4, workers=8) # vector_size = size\n",
    "    k = w2v.build_vocab(X_train)\n",
    "    w2v.train(X_train, total_examples=len(X_train), epochs=32)\n",
    "    vectors = w2v.wv\n",
    "\n",
    "    print(\"Processing training data...               \", end='\\r')\n",
    "    \n",
    "    new_X_train = []\n",
    "    train_size = len(X_train)\n",
    "    num = 0\n",
    "    for words in X_train:\n",
    "        if(num % 5000 == 0):\n",
    "            print(\"Training: %d/%d          \" %(num,train_size), end='\\r')\n",
    "            \n",
    "        temp = []\n",
    "        for word in words:\n",
    "            if word in vectors:\n",
    "                temp.append(vectors[word])\n",
    "        if temp:\n",
    "            doc_vector = np.mean(temp, axis=0).tolist()\n",
    "        else:\n",
    "            doc_vector = [np.nan] * 200\n",
    "\n",
    "        new_X_train.append(doc_vector)\n",
    "        num += 1\n",
    "        \n",
    "    X_train = pd.DataFrame(new_X_train)\n",
    "    \n",
    "    print(\"Processing testing data...   \",end='\\r')\n",
    "    new_X_test = []\n",
    "    num = 0\n",
    "    test_size = len(X_test)\n",
    "    for words in X_test:\n",
    "        if(num % 5000 == 0):\n",
    "            print(\"Testing: %d/%d          \" %(num,test_size), end='\\r')\n",
    "        \n",
    "        temp = []\n",
    "        for word in words:\n",
    "            if word in vectors:\n",
    "                temp.append(vectors[word])\n",
    "        if temp:\n",
    "            doc_vector = np.mean(temp, axis=0).tolist()\n",
    "        else:\n",
    "            doc_vector = [np.nan] * 200\n",
    "\n",
    "        new_X_test.append(doc_vector)\n",
    "        num += 1\n",
    "        \n",
    "    X_test = pd.DataFrame(new_X_test)\n",
    "\n",
    "    X_train['label'] = y_train\n",
    "    X_test['label'] = y_test\n",
    "\n",
    "    X_train = X_train.dropna()\n",
    "    y_train = X_train['label']\n",
    "    drop_labels = ['label']\n",
    "    X_train = X_train.drop(columns=drop_labels)\n",
    "    y_train = np.array([max(i, 0) for i in y_train])\n",
    "\n",
    "    X_test = X_test.dropna()\n",
    "    y_test = X_test['label']\n",
    "    X_test = X_test.drop(columns=drop_labels)\n",
    "    y_test = np.array([max(i, 0) for i in y_test])\n",
    "    \n",
    "\n",
    "    print(\"Building neural network....       \", end='\\r')\n",
    "    model = Sequential([\n",
    "        Dense(2048, activation='relu', input_shape=(200,)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=['accuracy']\n",
    "                  )\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    print(\"Training neural network....       \", end='\\r')\n",
    "    model.fit(X_train, y_train,\n",
    "              validation_data=(X_test, y_test),\n",
    "              epochs=100, callbacks=[es], verbose=0)\n",
    "    \n",
    "    return vectors, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "972e6e9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T23:31:01.106156Z",
     "start_time": "2021-11-24T23:31:01.102174Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_text(vectors, input_text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(input_text)\n",
    "    \n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    input_text = ' '.join(words)\n",
    "    \n",
    "    new_X_train = []\n",
    "    num = 0\n",
    "    for words in [input_text.split()]:\n",
    "        temp = []\n",
    "        for word in words:\n",
    "            if word in vectors:\n",
    "                temp.append(vectors[word])\n",
    "        if temp:\n",
    "            doc_vector = np.mean(temp, axis=0).tolist()\n",
    "\n",
    "        new_X_train.append(doc_vector)\n",
    "        num += 1\n",
    "        \n",
    "    X_train = pd.DataFrame(new_X_train).dropna()\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a622534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T23:31:01.111558Z",
     "start_time": "2021-11-24T23:31:01.107908Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    vectors, model = initialize()\n",
    "    \n",
    "    count = 0\n",
    "    while count < 500:\n",
    "        input_text = input(\"Tell a joke (type e to exit): \")\n",
    "        if not input_text:\n",
    "            input_text = \"\"\n",
    "            print(\"Empty Input\")\n",
    "        elif input_text.lower() in ['e','exit']:\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        else:\n",
    "            embedding = process_text(vectors, input_text)\n",
    "            pred = model.predict(embedding)\n",
    "            pred = np.round(pred)[0,0] \n",
    "\n",
    "            if pred > 0:\n",
    "                print(\"Joke\\n\")\n",
    "            else:\n",
    "                print(\"Headline\\n\")\n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0654aa9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-24T23:30:57.123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network....               \r"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab9ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
